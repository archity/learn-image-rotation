{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vb_techtest_notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtwNLAkylszy",
        "outputId": "19e820be-caed-425f-a7bd-72c4db1b623b"
      },
      "source": [
        "# !pip install torchnet\n",
        "!pip install torch==1.8.1 torchvision==0.1.8\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchnet as tnt\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "from PIL import Image\n",
        "import os\n",
        "import errno\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "_CIFAR_DATASET_DIR = './cifar-10'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (1.8.1)\n",
            "Requirement already satisfied: torchvision==0.1.8 in /usr/local/lib/python3.7/dist-packages (0.1.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from torchvision==0.1.8) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.1.8) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5_Augz6AhgK",
        "outputId": "5fdbc27e-6881-45e6-8af9-45ccb66f7f97"
      },
      "source": [
        "print(torch.__version__)\n",
        "# print(torchvision.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.8.1+cu102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY-adr9jzAG1"
      },
      "source": [
        "class Denormalize(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        for t, m, s in zip(tensor, self.mean, self.std):\n",
        "            t.mul_(s).add_(m)\n",
        "        return tensor\n",
        "\n",
        "def rotate_img(img, rot):\n",
        "    if rot == 0: # 0 degrees rotation\n",
        "        return img\n",
        "    elif rot == 90: # 90 degrees rotation\n",
        "        return np.flipud(np.transpose(img, (1,0,2)))\n",
        "    elif rot == 180: # 90 degrees rotation\n",
        "        return np.fliplr(np.flipud(img))\n",
        "    elif rot == 270: # 270 degrees rotation / or -90\n",
        "        return np.transpose(np.flipud(img), (1,0,2))\n",
        "    else:\n",
        "        raise ValueError('rotation should be 0, 90, 180, or 270 degrees')\n",
        "\n",
        "\n",
        "class GenericDataset(data.Dataset):\n",
        "    def __init__(self, dataset_name, split, random_sized_crop=False,\n",
        "                 num_imgs_per_cat=None):\n",
        "        self.split = split.lower()\n",
        "        self.dataset_name =  dataset_name.lower()\n",
        "        self.name = self.dataset_name + '_' + self.split\n",
        "        self.random_sized_crop = random_sized_crop\n",
        "\n",
        "        # The num_imgs_per_cats input argument specifies the number\n",
        "        # of training examples per category that would be used.\n",
        "        # This input argument was introduced in order to be able\n",
        "        # to use less annotated examples than what are available\n",
        "        # in a semi-superivsed experiment. By default all the \n",
        "        # available training examplers per category are being used.\n",
        "        self.num_imgs_per_cat = num_imgs_per_cat\n",
        "\n",
        "        if self.dataset_name=='imagenet':\n",
        "            assert(self.split=='train' or self.split=='val')\n",
        "            self.mean_pix = [0.485, 0.456, 0.406]\n",
        "            self.std_pix = [0.229, 0.224, 0.225]\n",
        "\n",
        "            if self.split!='train':\n",
        "                transforms_list = [\n",
        "                    transforms.Scale(256),\n",
        "                    transforms.CenterCrop(224),\n",
        "                    lambda x: np.asarray(x),\n",
        "                ]\n",
        "            else:\n",
        "                if self.random_sized_crop:\n",
        "                    transforms_list = [\n",
        "                        transforms.RandomSizedCrop(224),\n",
        "                        transforms.RandomHorizontalFlip(),\n",
        "                        lambda x: np.asarray(x),\n",
        "                    ]\n",
        "                else:\n",
        "                    transforms_list = [\n",
        "                        transforms.Scale(256),\n",
        "                        transforms.RandomCrop(224),\n",
        "                        transforms.RandomHorizontalFlip(),\n",
        "                        lambda x: np.asarray(x),\n",
        "                    ]\n",
        "            self.transform = transforms.Compose(transforms_list)\n",
        "            split_data_dir = _IMAGENET_DATASET_DIR + '/' + self.split\n",
        "            self.data = datasets.ImageFolder(split_data_dir, self.transform)\n",
        "        elif self.dataset_name=='places205':\n",
        "            self.mean_pix = [0.485, 0.456, 0.406]\n",
        "            self.std_pix = [0.229, 0.224, 0.225]\n",
        "            if self.split!='train':\n",
        "                transforms_list = [\n",
        "                    transforms.CenterCrop(224),\n",
        "                    lambda x: np.asarray(x),\n",
        "                ]\n",
        "            else:\n",
        "                if self.random_sized_crop:\n",
        "                    transforms_list = [\n",
        "                        transforms.RandomSizedCrop(224),\n",
        "                        transforms.RandomHorizontalFlip(),\n",
        "                        lambda x: np.asarray(x),\n",
        "                    ]\n",
        "                else:\n",
        "                    transforms_list = [\n",
        "                        transforms.RandomCrop(224),\n",
        "                        transforms.RandomHorizontalFlip(),\n",
        "                        lambda x: np.asarray(x),\n",
        "                    ]\n",
        "            self.transform = transforms.Compose(transforms_list)\n",
        "            self.data = Places205(root=_PLACES205_DATASET_DIR, split=self.split,\n",
        "                transform=self.transform)\n",
        "        elif self.dataset_name=='cifar10':\n",
        "            self.mean_pix = [x/255.0 for x in [125.3, 123.0, 113.9]]\n",
        "            self.std_pix = [x/255.0 for x in [63.0, 62.1, 66.7]]\n",
        "\n",
        "            if self.random_sized_crop:\n",
        "                raise ValueError('The random size crop option is not supported for the CIFAR dataset')\n",
        "\n",
        "            transform = []\n",
        "            if (split != 'test'):\n",
        "                transform.append(transforms.RandomCrop(32, padding=4))\n",
        "                transform.append(transforms.RandomHorizontalFlip())\n",
        "            transform.append(lambda x: np.asarray(x))\n",
        "            self.transform = transforms.Compose(transform)\n",
        "            self.data = datasets.__dict__[self.dataset_name.upper()](\n",
        "                _CIFAR_DATASET_DIR, train=self.split=='train',\n",
        "                download=True, transform=self.transform)\n",
        "        else:\n",
        "            raise ValueError('Not recognized dataset {0}'.format(dname))\n",
        "        \n",
        "        if num_imgs_per_cat is not None:\n",
        "            self._keep_first_k_examples_per_category(num_imgs_per_cat)\n",
        "        \n",
        "    \n",
        "    def _keep_first_k_examples_per_category(self, num_imgs_per_cat):\n",
        "        print('num_imgs_per_category {0}'.format(num_imgs_per_cat))\n",
        "   \n",
        "        if self.dataset_name=='cifar10':\n",
        "            labels = self.data.test_labels if (self.split=='test') else self.data.train_labels\n",
        "            data = self.data.test_data if (self.split=='test') else self.data.train_data\n",
        "            label2ind = buildLabelIndex(labels)\n",
        "            all_indices = []\n",
        "            for cat in label2ind.keys():\n",
        "                label2ind[cat] = label2ind[cat][:num_imgs_per_cat]\n",
        "                all_indices += label2ind[cat]\n",
        "            all_indices = sorted(all_indices)\n",
        "            data = data[all_indices]\n",
        "            labels = [labels[idx] for idx in all_indices]\n",
        "            if self.split=='test':\n",
        "                self.data.test_labels = labels\n",
        "                self.data.test_data = data\n",
        "            else: \n",
        "                self.data.train_labels = labels\n",
        "                self.data.train_data = data\n",
        "\n",
        "            label2ind = buildLabelIndex(labels)\n",
        "            for k, v in label2ind.items(): \n",
        "                assert(len(v)==num_imgs_per_cat)\n",
        "\n",
        "        elif self.dataset_name=='imagenet':\n",
        "            raise ValueError('Keeping k examples per category has not been implemented for the {0}'.format(dname))\n",
        "        elif self.dataset_name=='place205':\n",
        "            raise ValueError('Keeping k examples per category has not been implemented for the {0}'.format(dname))\n",
        "        else:\n",
        "            raise ValueError('Not recognized dataset {0}'.format(dname))\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, label = self.data[index]\n",
        "        return img, int(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "class DataLoader(object):\n",
        "    def __init__(self,\n",
        "                 dataset,\n",
        "                 batch_size=1,\n",
        "                 unsupervised=True,\n",
        "                 epoch_size=None,\n",
        "                 num_workers=0,\n",
        "                 shuffle=True):\n",
        "        self.dataset = dataset\n",
        "        self.shuffle = shuffle\n",
        "        self.epoch_size = epoch_size if epoch_size is not None else len(dataset)\n",
        "        self.batch_size = batch_size\n",
        "        self.unsupervised = unsupervised\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "        mean_pix  = self.dataset.mean_pix\n",
        "        std_pix   = self.dataset.std_pix\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=mean_pix, std=std_pix)\n",
        "        ])\n",
        "        self.inv_transform = transforms.Compose([\n",
        "            Denormalize(mean_pix, std_pix),\n",
        "            lambda x: x.numpy() * 255.0,\n",
        "            lambda x: x.transpose(1,2,0).astype(np.uint8),\n",
        "        ])\n",
        "\n",
        "    def get_iterator(self, epoch=0):\n",
        "        rand_seed = epoch * self.epoch_size\n",
        "        random.seed(rand_seed)\n",
        "        if self.unsupervised:\n",
        "            # if in unsupervised mode define a loader function that given the\n",
        "            # index of an image it returns the 4 rotated copies of the image\n",
        "            # plus the label of the rotation, i.e., 0 for 0 degrees rotation,\n",
        "            # 1 for 90 degrees, 2 for 180 degrees, and 3 for 270 degrees.\n",
        "            def _load_function(idx):\n",
        "                idx = idx % len(self.dataset)\n",
        "                img0, _ = self.dataset[idx]\n",
        "                rotated_imgs = [\n",
        "                    self.transform(img0),\n",
        "                    self.transform(rotate_img(img0,  90)),\n",
        "                    self.transform(rotate_img(img0, 180)),\n",
        "                    self.transform(rotate_img(img0, 270))\n",
        "                ]\n",
        "                rotation_labels = torch.LongTensor([0, 1, 2, 3])\n",
        "                return torch.stack(rotated_imgs, dim=0), rotation_labels\n",
        "            def _collate_fun(batch):\n",
        "                batch = default_collate(batch)\n",
        "                assert(len(batch)==2)\n",
        "                batch_size, rotations, channels, height, width = batch[0].size()\n",
        "                batch[0] = batch[0].view([batch_size*rotations, channels, height, width])\n",
        "                batch[1] = batch[1].view([batch_size*rotations])\n",
        "                return batch\n",
        "        else: # supervised mode\n",
        "            # if in supervised mode define a loader function that given the\n",
        "            # index of an image it returns the image and its categorical label\n",
        "            def _load_function(idx):\n",
        "                idx = idx % len(self.dataset)\n",
        "                img, categorical_label = self.dataset[idx]\n",
        "                img = self.transform(img)\n",
        "                return img, categorical_label\n",
        "            _collate_fun = default_collate\n",
        "\n",
        "        tnt_dataset = tnt.dataset.ListDataset(elem_list=range(self.epoch_size),\n",
        "            load=_load_function)\n",
        "        data_loader = tnt_dataset.parallel(batch_size=self.batch_size,\n",
        "            collate_fn=_collate_fun, num_workers=self.num_workers,\n",
        "            shuffle=self.shuffle)\n",
        "        return data_loader\n",
        "\n",
        "    def __call__(self, epoch=0):\n",
        "        return self.get_iterator(epoch)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.epoch_size / self.batch_size"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "t3skKWCDzKYt",
        "outputId": "e7d1b798-cf0b-47f0-bb81-8eb61519ec90"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "dataset = GenericDataset('cifar10','train', random_sized_crop=False)\n",
        "dataloader = DataLoader(dataset, batch_size=8, unsupervised=True)\n",
        "\n",
        "for b in dataloader(0):\n",
        "        data, label = b\n",
        "        break\n",
        "\n",
        "inv_transform = dataloader.inv_transform\n",
        "for i in range(data.size(0)):\n",
        "    plt.subplot(data.size(0)/4,4,i+1)\n",
        "    fig=plt.imshow(inv_transform(data[i]))\n",
        "    fig.axes.get_xaxis().set_visible(False)\n",
        "    fig.axes.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-e97ea1b7f186>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsupervised\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchnet/dataset/listdataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s/%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-f4d9b880cc93>\u001b[0m in \u001b[0;36m_load_function\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m    195\u001b[0m                 rotated_imgs = [\n\u001b[1;32m    196\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotate_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotate_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m180\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrotate_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m270\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# handle numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;31m# pic = np.ascontiguousarray(pic, dtype=np.float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mpic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: At least one stride in the given numpy array is negative, and tensors with negative strides are not currently supported. (You can probably work around this by making a copy of your array  with array.copy().) "
          ]
        }
      ]
    }
  ]
}